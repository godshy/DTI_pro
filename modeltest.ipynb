{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Training dataset...\n",
      "Loading labels: train_interaction.npy\n",
      "Loading chemIDs: train_chemIDs.npy\n",
      "Loading proIDs: train_proIDs.txt\n",
      "Loading sequences: train_reprotein.npy\n",
      "interactions.shape:  (14196, 1) ecfp.shape:  (14196, 1024) sequences.shape:  (14196, 1, 5762, 20) n2vc.shape: (14196, 128) n2vp.shape: (14196, 128)\n"
     ]
    }
   ],
   "source": [
    "#  making feature vectors of seq, one-hot encoding\n",
    "\n",
    "print('Making Training dataset...')\n",
    "ecfp = np.load('./dataset_hard'+'/cv_'+str(0)+'/train_fingerprint.npy')\n",
    "ecfp = np.asarray(ecfp, dtype='float32').reshape(-1,1024)\n",
    "\n",
    "file_interactions=np.load('./dataset_hard'+'/cv_'+str(0)+'/train_interaction.npy')\n",
    "print('Loading labels: train_interaction.npy')\n",
    "cID = np.load('./dataset_hard'+'/cv_'+str(0)+'/train_chemIDs.npy')\n",
    "print('Loading chemIDs: train_chemIDs.npy')\n",
    "with open('./dataset_hard'+'/cv_'+str(0)+'/train_proIDs.txt') as f:\n",
    "    pID = [s.strip() for s in f.readlines()]\n",
    "print('Loading proIDs: train_proIDs.txt')\n",
    "n2v_c, n2v_p = [], []\n",
    "with open('./modelpp.pickle', mode='rb') as f:\n",
    "    modelpp = pickle.load(f)\n",
    "with open('./modelcc.pickle', mode='rb') as f:\n",
    "    modelcc = pickle.load(f)\n",
    "for j in cID:\n",
    "    n2v_c.append(modelcc.wv[str(j)])\n",
    "for k in pID:\n",
    "    n2v_p.append(modelpp.wv[k])\n",
    "interactions = np.asarray(file_interactions, dtype='int32').reshape(-1,1)\n",
    "n2vc = np.asarray(n2v_c, dtype='float32').reshape(-1, 128)\n",
    "n2vp = np.asarray(n2v_p, dtype='float32').reshape(-1, 128)\n",
    "#reset memory\n",
    "del n2v_c, n2v_p, cID, pID, modelcc, modelpp, file_interactions\n",
    "gc.collect()\n",
    "\n",
    "file_sequences=np.load('./dataset_hard'+'/cv_'+str(0)+'/train_reprotein.npy')\n",
    "print('Loading sequences: train_reprotein.npy', flush=True)\n",
    "sequences = np.asarray(file_sequences, dtype='float32').reshape((-1, 1, 5762, 20))\n",
    "# reset memory\n",
    "del file_sequences\n",
    "gc.collect()\n",
    "\n",
    "print('interactions.shape: ', interactions.shape, 'ecfp.shape: ', ecfp.shape,'sequences.shape: ',  sequences.shape, 'n2vc.shape:', n2vc.shape,'n2vp.shape:', n2vp.shape, flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('use', device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "DeepCNN(\n  (conv1_pro): Conv2d(1, 64, kernel_size=(33, 20), stride=(1,), padding=(16, 0))\n  (bn1_pro): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2_pro): Conv2d(64, 64, kernel_size=(23, 1), stride=(1,), padding=(11, 0))\n  (bn2_pro): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3_pro): Conv2d(64, 32, kernel_size=(33, 1), stride=(1,), padding=(16, 0))\n  (bn3_pro): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import ignite.metrics\n",
    "\n",
    "# prosize: 5762, plensize:20\n",
    "# j1:33, s1:1, pf1:64 = window-size, stride-step, No. of filters of first protein-CNN convolution layer\n",
    "# ja1:17 sa1:1 = window-size, stride-step of first protein-CNN average-pooling layer\n",
    "# j2:23,s2:1, pf2:64 = second protein-CNN convolution layer\n",
    "# ja2:11, sa2:1 = second protein-CNN average-pooling layer\n",
    "# j3:33, s3:1, pf3:32 = third protein-CNN convolution layer\n",
    "# ja3:17, sa3:1 third protein-CNN average-pooling layer\n",
    "# n_hid3:70, n_hid4:80, n_hid5:60, n_out:1\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        # first conv of seq_cnn\n",
    "        self.conv1_pro = nn.Conv2d(1, 64, (33, 20), stride= (1, ), padding=(33//2, 0))\n",
    "        self.bn1_pro = nn.BatchNorm2d(64)\n",
    "        # second conv of seq_cnn\n",
    "        self.conv2_pro = nn.Conv2d(64, 64, (23, 1), stride= (1, ), padding=(23//2, 0))\n",
    "        self.bn2_pro = nn.BatchNorm2d(64)\n",
    "        # third conv of seq_cnn\n",
    "        self.conv3_pro = nn.Conv2d(64, 32, (33, 1), stride=(1, ), padding=(33//2, 0))\n",
    "        self.bn3_pro = nn.BatchNorm2d(32)\n",
    "        self.fc3_pro = nn.Linear(1, 70)\n",
    "        self.fc4 = nn.Linear(1152, 80) # 1024 + 128\n",
    "        self.fc5 = nn.Linear(80, 60) # nhid4, nhid5\n",
    "        self.fc4_pro = nn.Linear(2368, 80) # 2240+128\n",
    "        self.fc5_pro = nn.Linear(80, 60)\n",
    "        self.fc6 = nn.Linear(3600, 1)  #\n",
    "\n",
    "\n",
    "        self.m1 = (5762+(33//2*2)-33)//1+1\n",
    "        # print('m1', self.m1)\n",
    "        self.m2 = (self.m1+(17//2*2)-17)//1+1\n",
    "        # print('m2', self.m2)\n",
    "        self.m3 = (self.m2+(23//2*2)-23)//1+1\n",
    "        # print('m3', self.m3)\n",
    "        self.m4 = (self.m3+(11//2*2)-11)//1+1\n",
    "        # print('m4', self.m4)\n",
    "        self.m5 = (self.m4+(33//2*2)-33)//1+1\n",
    "        # print('m5', self.m5)\n",
    "        self.m6 = (self.m5+(17//2*2)-17)//1+1\n",
    "        # print('m6', self.m6)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq = self.conv1_pro(seq)  # first conv\n",
    "        seq = self.bn1_pro(seq)    # batch norm\n",
    "        seq = F.leaky_relu(seq)    # leaky_relu activation\n",
    "        seq = F.dropout(seq, p=0.2) # dropout\n",
    "        seq = F.avg_pool2d(seq, (17, 1), stride=1, padding=(17//2, 0)) # avg_pooling\n",
    "\n",
    "        seq = self.conv2_pro(seq)\n",
    "        seq = self.bn2_pro(seq)\n",
    "        seq = F.leaky_relu(seq)\n",
    "        seq = F.dropout(seq, p=0.2)\n",
    "        seq = F.avg_pool2d(seq, (11, 1), stride=1, padding=(11//2, 0))\n",
    "\n",
    "        seq = self.conv3_pro(seq)\n",
    "        seq = self.bn3_pro(seq)\n",
    "        seq = F.leaky_relu(seq)\n",
    "        seq = F.dropout(seq, p=0.2)\n",
    "        seq = F.avg_pool2d(seq, (17, 1), stride=1, padding=(17//2, 0))\n",
    "        seq_protein = F.max_pool2d(seq, (self.m6, 1))\n",
    "        # fully-connect fc3\n",
    "        seq_protein = F.leaky_relu(self.fc3_pro(seq_protein))\n",
    "        seq_protein = F.dropout(seq_protein, p=0.2)\n",
    "        return seq_protein\n",
    "\n",
    "    def cos_similarity(self, fp, seq_, n2c, n2p):\n",
    "        x_compound = fp\n",
    "        x_compound = self.fc4(torch.concat((x_compound, n2c)))\n",
    "        x_compound = F.dropout(F.leaky_relu(x_compound), p=0.2)\n",
    "        x_compound = F.dropout(F.leaky_relu(self.fc5(x_compound)), p=0.2)\n",
    "        x_protein = self.predict_pro(seq_)\n",
    "        x_protein = self.fc4_pro(torch.cat((x_protein, n2p)))\n",
    "        x_protein = F.dropout(F.leaky_relu(x_protein), p=0.2)\n",
    "        #print(x_protein.shape)\n",
    "        x_protein = F.dropout(F.leaky_relu(self.fc5_pro(x_protein)), p=0.2)\n",
    "        #print(x_protein.shape)\n",
    "        y = x_compound * x_protein\n",
    "        return y\n",
    "\n",
    "    def __call__(self, fp, seq_, n2c, n2p, interaction):\n",
    "        z = self.cos_similarity(ecfp, sequences, n2vc, n2vp)\n",
    "        print('Z shape:', z.shape)\n",
    "        Z = self.fc6(z)\n",
    "\n",
    "        loss = F.cosine_similarity(Z, interactions)\n",
    "        # loss = tf.compat.v1.losses.sigmoid_cross_entropy(Z, interactions)\n",
    "        # ---------------------------------------------------------------\n",
    "        accuracy = ignite.metrics.Accuracy(Z, interactions)\n",
    "        # accuracy_ = tf.keras.metrics.binary_accuracy(Z, interactions) #---\n",
    "        # ---------------------------------------------------------------\n",
    "        print({'loss': loss, 'accuracy': accuracy}, self)\n",
    "        return loss\n",
    "\n",
    "\n",
    "model = DeepCNN()\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14196, 32)\n",
      "Total time is 39.66327261924744 sec．\n"
     ]
    }
   ],
   "source": [
    "START = time.time()\n",
    "features = []\n",
    "for i in range(14196):\n",
    "    seq_pro = torch.from_numpy(sequences[i].astype(np.float32)).clone()\n",
    "    seq_pro = seq_pro.reshape(1, 1, 5762, 20)\n",
    "    #print(seq.shape)\n",
    "    seq = seq_pro.to(device)\n",
    "    with torch.no_grad():\n",
    "        feature = model(seq)\n",
    "    features.append(feature.cpu().detach().numpy().reshape(-1))\n",
    "features = np.array(features)\n",
    "print(features.shape)\n",
    "\n",
    "END = time.time()\n",
    "print('Total time is {} sec．'.format(END-START))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'chainer.datasets.tuple_dataset.TupleDataset'>\n",
      "5\n",
      "(array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32), array([ 0.72283   ,  0.3805213 , -0.0905413 ,  0.39402148,  0.4876889 ,\n",
      "       -0.09808945,  0.31464857,  0.07862262, -0.29625794,  0.4016296 ,\n",
      "       -0.01218679, -0.50566816,  0.26562607, -0.10225522,  0.3340107 ,\n",
      "        0.23728873,  0.02845587, -0.06437396,  0.00943288,  0.31212628,\n",
      "       -0.16052926,  0.44658536, -0.13911773,  0.23581393,  0.05283666,\n",
      "        0.36045703,  0.5515379 ,  0.23126523, -0.00510399, -0.29908812,\n",
      "       -0.06842223,  0.19955993,  0.07587766, -0.83031344, -0.28564745,\n",
      "        0.2915692 , -0.1540279 , -0.41768283, -0.18621375,  0.3890175 ,\n",
      "        0.02277567,  0.41691542,  0.2373718 , -0.06897342,  0.40342778,\n",
      "        0.18830715, -0.02489281, -0.11925767,  0.55899227,  0.15472819,\n",
      "       -0.04291795, -0.05463634,  0.12092942, -0.20436889, -0.6673109 ,\n",
      "       -0.45851836,  0.29230013, -0.45586237, -0.25063124,  0.639119  ,\n",
      "       -0.44493684,  0.03245541,  0.34358233, -0.06124233,  0.27209947,\n",
      "        0.25235465, -0.15179878,  0.03529884, -0.22966474, -0.0069883 ,\n",
      "       -0.11131263, -0.10612431, -0.1781765 , -0.42075455,  0.5133923 ,\n",
      "        0.12545688, -0.67714083,  0.35155144, -0.25024933,  0.25971538,\n",
      "       -0.20378138,  0.07746973,  0.56675696,  0.10457854, -0.0284071 ,\n",
      "        0.38338587, -0.13533081, -0.08441786,  0.02554796,  0.06434391,\n",
      "        0.54471207, -0.2536574 ,  0.0625247 ,  0.07220387,  0.06600185,\n",
      "        0.09661449,  0.1795118 , -0.37643906,  0.13263091,  0.1555048 ,\n",
      "        0.20178074,  0.28872436, -0.05746845,  0.27562815, -0.25890625,\n",
      "       -0.16455464, -0.07000495,  0.39696434, -0.12248171, -0.06360547,\n",
      "        0.02763982,  0.2189543 , -0.1021452 ,  0.35115588, -0.25452158,\n",
      "        0.05530042,  0.85916656, -0.10147172, -0.06268831,  0.01284577,\n",
      "       -0.20754091, -0.61031014, -0.26239967, -0.02602741,  0.20887573,\n",
      "        0.11024396,  0.15438476, -0.08605073], dtype=float32), array([ 0.34740075, -0.18292448, -0.0902673 ,  0.17398825,  0.21830389,\n",
      "       -0.35708028,  0.17481129, -0.06206405,  0.0863728 , -0.10548431,\n",
      "        0.03699968,  0.03210669, -0.12447275, -0.21030897,  0.09504067,\n",
      "        0.42459387, -0.13075338,  0.09956617,  0.11437931,  0.39801466,\n",
      "        0.03433449,  0.17103013, -0.051882  ,  0.17094117, -0.07074516,\n",
      "       -0.06207346, -0.37196684, -0.10825807,  0.10509187,  0.06728826,\n",
      "        0.10948929, -0.08418851,  0.4417109 ,  0.19448696, -0.3144243 ,\n",
      "       -0.15614952,  0.09766671, -0.03362546,  0.06528192, -0.15115419,\n",
      "        0.23144329,  0.06146125,  0.0172526 ,  0.131839  ,  0.4028212 ,\n",
      "       -0.27793804, -0.19265129, -0.14435908, -0.10213514, -0.03627249,\n",
      "       -0.06191694, -0.12924744,  0.17151651,  0.11081165, -0.05607855,\n",
      "        0.53995883,  0.036068  , -0.08813745,  0.0380383 , -0.19947793,\n",
      "       -0.02991767,  0.37163234,  0.04780305, -0.01309089,  0.5598723 ,\n",
      "       -0.07185788, -0.06955337, -0.27350473,  0.3400835 , -0.38512218,\n",
      "        0.22198915, -0.11615139, -0.13881466, -0.11577377, -0.02239755,\n",
      "       -0.24063426, -0.48640496, -0.21747969, -0.17548391,  0.32651553,\n",
      "       -0.04201664, -0.45576006, -0.06395359,  0.36872953,  0.36604375,\n",
      "       -0.11975751,  0.15362912, -0.12752737, -0.15400967,  0.07420179,\n",
      "       -0.15642273, -0.05813711, -0.0232537 ,  0.20786138, -0.01182907,\n",
      "       -0.20468858, -0.1475866 ,  0.40264508,  0.34277964,  0.08408991,\n",
      "       -0.41335592, -0.23193333,  0.03614735, -0.2594587 , -0.07160264,\n",
      "        0.09257665, -0.17793082,  0.18442073,  0.4298758 ,  0.07471211,\n",
      "       -0.17909463,  0.1562206 ,  0.24505964, -0.0651199 ,  0.11954673,\n",
      "       -0.02458427, -0.16820617,  0.11099654,  0.08687449, -0.02986359,\n",
      "       -0.30292484, -0.14998662, -0.08620984,  0.30022636, -0.36129022,\n",
      "        0.07361061, -0.46096987,  0.2548905 ], dtype=float32), array([1]))\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "#import chainer.functions as F\n",
    "#from chainer import datasets\n",
    "# print(ecfp.shape, n2vc.shape)\n",
    "\n",
    "# train_dataset = datasets.TupleDataset(ecfp, sequences, n2vc, n2vp, interactions)\n",
    "# train_dataset in chainer:\n",
    "# (ecfp, sequences, n2vc, n2vp, interactions)\n",
    "# ...\n",
    "# (ecfp, sequences, n2vc, n2vp, interactions)  14196 x 5\n",
    "dataset_pytorch = []\n",
    "\n",
    "for i in range(14196):\n",
    "    dataset_pytorch.append((torch.from_numpy(ecfp[i, :].astype(np.float32)).clone(),\n",
    "                            torch.from_numpy(sequences[i].astype(np.float32)).clone(),\n",
    "                            torch.from_numpy(n2vc[i, :].astype(np.float32)).clone(),\n",
    "                            torch.from_numpy(n2vp[i, :].astype(np.float32)).clone(),\n",
    "                            torch.from_numpy(interactions[i].astype(np.float32)).clone()))\n",
    "\n",
    "# print(len(dataset_pytorch), len(dataset_pytorch[0]))\n",
    "\n",
    "n = int(0.8 * len(dataset_pytorch))\n",
    "train_dataset_pytorch, valid_dataset_pytorch = dataset_pytorch[:n], dataset_pytorch[n:]\n",
    "print('train: ', len(train_dataset_pytorch), flush=True)\n",
    "print('valid: ', len(valid_dataset_pytorch), flush=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset_pytorch, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(valid_dataset_pytorch, batch_size=100, shuffle=True)\n",
    "#ecfp_ = torch.from_numpy(ecfp.astype(np.float32)).clone()\n",
    "#n2vc_ = torch.from_numpy(n2vc.astype(np.float32)).clone()\n",
    "#a = torch.concat(ecfp_.to(device), n2vc_.to(device))\n",
    "# a = F.concat(ecfp, n2vc)\n",
    "\n",
    "# print(a.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator, Events\n",
    "\n",
    "# train initialize\n",
    "\n",
    "output_dir ='./result/dataset_hard'+'/'+'ecfpN2vc_mSGD'+'/'+'pattern'+str(0)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "#-------------------------------\n",
    "#reset memory again\n",
    "del sequences, interactions, ecfp, n2vc, n2vp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.00001)\n",
    "\n",
    "print('Trainer is setting up...', flush=True)\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, F.nll_loss, device=device)\n",
    "evaluator = create_supervised_evaluator(model, metrics={'accuracy': Accuracy(), 'nll': Loss(F.nll_loss)}, device=device)\n",
    "training_history = {'accuracy': [], 'loss': []}\n",
    "validation_history = {'accuracy': [], 'loss': []}\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(train_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll = metrics['nll']\n",
    "    training_history['accuracy'].append(avg_accuracy)\n",
    "    training_history['loss'].append(avg_nll)\n",
    "    print(\n",
    "        \"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "            .format(engine.state.epoch, avg_accuracy, avg_nll)\n",
    "    )\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(test_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    avg_accuracy = metrics['accuracy']\n",
    "    avg_nll = metrics['nll']\n",
    "    validation_history['accuracy'].append(avg_accuracy)\n",
    "    validation_history['loss'].append(avg_nll)\n",
    "    print(\n",
    "        \"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "            .format(engine.state.epoch, avg_accuracy, avg_nll))\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    './models',\n",
    "    'model',\n",
    "    save_interval=1,\n",
    "    n_saved=2,\n",
    "    create_dir=True,\n",
    "    save_as_state_dict=True,\n",
    "    require_empty=False,\n",
    ")\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'Model': model})\n",
    "\n",
    "del model, trainer\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-2acb6551",
   "language": "python",
   "display_name": "PyCharm (DTI_pro)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}