Traceback (most recent call last):
  File "train.py", line 254, in <module>
    app.run(main)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py", line 303, in run
    _run_main(main, args)
  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "train.py", line 201, in main
    wandb_logger.watch(model)
  File "/Users/shy/Library/Python/3.7/lib/python/site-packages/wandb/sdk/wandb_watch.py", line 95, in watch
    jupyter_run=wandb.run if in_jupyter else None,
  File "/Users/shy/Library/Python/3.7/lib/python/site-packages/wandb/wandb_torch.py", line 136, in add_log_hooks_to_pytorch_module
    parameter, "gradients/" + prefix + name, log_track_grad
  File "/Users/shy/Library/Python/3.7/lib/python/site-packages/wandb/wandb_torch.py", line 285, in _hook_variable_gradient_stats
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
  File "/Users/shy/Library/Python/3.7/lib/python/site-packages/torch/_tensor.py", line 339, in register_hook
    return handle_torch_function(Tensor.register_hook, (self,), self, hook)
  File "/Users/shy/Library/Python/3.7/lib/python/site-packages/torch/overrides.py", line 1355, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
  File "/Users/shy/Library/Python/3.7/lib/python/site-packages/torch/nn/parameter.py", line 126, in __torch_function__
    'to initialize the parameters before calling torch functions'.format(func, cls.__name__))
ValueError: Attempted to use an uninitialized parameter in <function Tensor.register_hook at 0x7f9c28574950>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions